\textit{Portions of this chapter were originally published in collaboration with Jake VanderPlas in the September 2018 edition of the Journal of Open Source Software (Fleming and VanderPlas 2018, JOSS, Vol. 3, 29, p. 781; 2018, DOI: 10.21105/joss.00781), and are reproduced below with permission of the Journal of Open Source Software.}

\section{\approxposterior}

\approxposterior is a Python package for efficient approximate Bayesian inference and Bayesian optimization of computationally-expensive models. \approxposterior trains a Gaussian process (GP) surrogate for the computationally-expensive model and employs an active learning approach to iteratively improve the GPs predictive performance while minimizing the number of calls to the expensive model required to generate the GP's training set.

\approxposterior implements both the Bayesian Active Learning for Posterior Estimation (BAPE, \citet{Kandasamy2017}) and Adaptive Gaussian process approximation for Bayesian inference with expensive likelihood functions (AGP, \citet{Wang2018}) algorithms for estimating posterior probability distributions for use with inference problems with computationally-expensive models. In such situations, the goal is to infer posterior probability distributions for model parameters, given some data, with the additional constraint of minimizing the number of forward model evaluations given the model's assumed large computational cost. \approxposterior trains a Gaussian Process (GP) surrogate model for the likelihood evaluation by modeling the covariances in logprobability (logprior + loglikelihood) space. \approxposterior then uses this GP within an MCMC sampler for each likelihood evaluation to perform the inference. \approxposterior iteratively improves the GP's predictive performance by leveraging the inherent uncertainty in the GP's predictions to identify high-likelihood regions in parameter space where the GP is uncertain. \approxposterior then evaluates the forward model at these points to expand the training set in relevant regions of parameter space, re-training the GP to maximize its predictive ability while minimizing the size of the training set. Check out the BAPE paper by \citet{Kandasamy2017} and the AGP paper by \citet{Wang2018} for in-depth descriptions of the respective algorithms.

TODO: move all explicit \approxposterior discussion from TRAPPIST-1 paper into this chapter