
\section{Combining Theory and Data to Learn About the Past}

Planets are inextricably linked to their host stars through their common birth environment. The fate of binary stars is similarly intertwined.  As such, numerous coupled astrophysical processes ranging from tidal forces \citep{Zahn1989,Barnes2017} to stellar high-energy emission \citep{Airapetian2019}, shape the long-term evolution of both stellar and planetary systems. Modern astronomical surveys have produced a wealth of information to help elucidate the processes that govern the evolution of stars and their planets. Notably, completed missions such as \kepler \citep{Borucki2003,Borucki2010} and \textit{K2} \citep{Howell2014}, and the in-progress Transiting Exoplanet Survey Satellite \citep[TESS, ][]{Ricker2014}, monitored the brightness variation of over 100,000 of stars to detect transiting exoplanets and eclipsing binary stars, massively increasing the known population of such systems.  These large amounts of data and the promise of future telescopes present an unprecedented opportunity for both population-level statistical studies and individual target characterization to infer the fundamental properties and processes that govern the evolution stars and their planets.

An unavoidable problem for these efforts within the discipline of astronomy, however, is that the timescales over which astrophysical processes, e.g. tides or atmospheric escape, operate are often much, much longer than the lifetime of a single astronomer, or even the duration of recorded human history. For example, the main sequence lifetime of Sun-like stars is of order 10 Gyrs \citep{Baraffe2015}, a factor of $10^7$ longer than an optimistic estimate for the lifetime of a person. How can astronomers hope to learn about the long-term evolution of such objects if they can only observe brief snapshots of the cosmos over time? The fundamental problem of astronomy therefore becomes, given what is observed, how can astronomers infer what physical processes produced the observations and what does this information imply about the past and future evolution? 

Theoretical models are fundamental to the interpretation of observed data and are required to understand the past and future evolution of astrophysical objects.  In cases where observations have not yet been made, e.g. the detection of a terrestrial exoplanetary atmosphere, theoretical models can be used to make predictions that will aid in the interpretation of future observations and aid in practical tasks such as target prioritization.  When future facilities like the James Webb Space Telescope (JWST) attempt to characterize terrestrial exoplanetary atmospheres in the search for biosignatures, for example, astronomers will use complex radiative transfer codes to estimate atmospheric compositions \citep[e.g. SMART;][]{Meadows1996,Crisp1997}. Critical to understanding and interpreting these results is determining how the exoplanet and its atmosphere evolved to its present state. This evolution necessarily depends on the long-term evolution of planet's host star because its high-energy XUV luminosity (X-ray and extreme ultraviolet emission ranging over approximately 1-1000\AA) drives volatile escape processes that have been shown to dramatically impact exoplanetary atmospheres \citep{Watson1981,Lammer2003,MurrayClay2009}. For example, \citet{Luger2015} showed via theory and simulation how the large pre-main sequence luminosities of low-mass stars drive water photolysis and loss from terrestrial exoplanets during an extended runaway greenhouse phase. This photolysis can cause extreme amounts of O$_2$ to build up in the planets' atmospheres, up to 1000s of bars, significantly impacting absorption features in their spectra and the prospects for identifying true positive biosignatures \citep[see][]{Meadows2017,Meadows2018}. Modeling the long-term evolution of astrophysical systems that consider the relevant physics is critical to the estimation of their present state.

Similarly for binary stars and any planets they might host, modeling is essential to understand what they were like long before the first astronomer. For example, binary stars with orbital periods less than about 10 days appear to preferentially exhibit circular orbits \citep[see][for a relatively recent observational study of this phenomenon]{Meibom2005}. 
xxx

Later in this dissertation, I further examine the consequences of tidal evolution in short-period binary stars.


When data are available, astronomers can infer information about astrophysical systems using models. Model predictions can be conditioned on the observations to derive constraints on parameters. That is, astronomers can attempt to match their model outputs with the observations within the uncertainties to constrain parameters. Many theoretical studies that examine the long-term evolution of systems to interpret observations and constrain parameters do so in the form of ``best fit models" by finding the set of model parameters that most closely reproduce the observations.  Another common technique is performing a grid search through parameter space to examine how model predictions vary as a function of model parameters.  For example, \citet{Ribas2016} and \citet{Barnes2016} both employed the aforementioned techniques when examining the long-term dynamical and atmospheric evolution of Proxima Centauri b to estimate its present-day potential habitability.  These techniques are useful and informative, but ultimately insufficient, however, as a robust analysis used to interpret observations must be based in a probabilistic framework in which model predictions are conditioned on the data. Moreover, any constrained parameter requires an uncertainty estimation to permit a credible interpretation of the results.  For example, if a forward model yields a best fit prediction of 1 Earth ocean of liquid surface water remaining on an exoplanet, one might infer that this exoplanet could be like Earth, however, how credible is this interpretation.  If instead, in this example, one propagated uncertainties through the forward model given the data and its uncertainties to find that the remaining surface water content is 1$^{+10}_{-1}$ Earth oceans, the planet's current state ranges from desiccated to water-rich, with both cases entirely consistent with the observations, yet indicative of significantly different evolutionary histories. Similarly analogies can be made for any quantity an astronomer would like to measure, e.g. stellar masses, ages, binary orbital eccentricities, etc.

To address this issue, one can use theoretical models to constrain unobserved parameters using the statistical method of Bayesian inference. Generally, Bayesian inference proceeds as follows:  One seeks to derive a probability distribution for model parameters, e.g. a star's mass given its observed luminosity.  That distribution should quantify how likely it is that the parameter takes on certain values and capture the inherent uncertainty in the parameter values given data.  This distribution is known as the ``posterior probability distribution" and accounts for observed data, the data uncertainties, parameter correlations, and one's prior belief about how the parameters are distributed. To compute the posterior distribution of model parameters, \textbf{x}, given observed data, $Data$, one applies Bayes' Theorem: 
\begin{equation} \label{intro:eqn:bayes}
P(\textbf{x} | Data) \propto P(Data | \textbf{x}) P(\textbf{x}),
\end{equation}
where $P(\textbf{x} | Data)$ is the posterior probability of \textbf{x} given $Data$, $P(\textbf{x})$ is the prior probability assigned to \textbf{x}, and $P(Data | \textbf{x})$ is the probability of the data given \textbf{x}, commonly referred to as the likelihood of $Data$ given \textbf{x}. This equation neglects the normalization constant, $1/P(Data)$, as this is typically quite difficult to compute because it requires integrating the likelihood over the prior. Through Bayes' Theorem, the posterior probability can be thought of as using data to update one's prior belief of how $\textbf{x}$ is distributed, given a model for the likelihood of the data. Note that in this dissertation, I will use the natural logarithm of Bayes' Theorem and refer to $\ln P(\textbf{x} | Data)$ to as the ``lnprobability". For notational convenience, I define the lnprobability as $f(\textbf{x})$. 

In this dissertation, I build theoretical models for the long-term evolution of single and binary stars and consider what impact this evolution has on the stars' planets given the available data. I implement these models in \vplanet\footnote{VPLanet is publicly available
at \href{https://github.com/VirtualPlanetaryLaboratory/vplanet}{{https://github.com/VirtualPlanetaryLaboratory/vplanet}}.}, a modular, open-source C code used to simulate the coupled evolution of stellar and exoplanetary systems subject to a wide range of coupled astrophysical processes \citep{Barnes2019}. When performing Bayesian inference with \vplanet, I must compute the likelihood by running a simulation to compute a prediction, e.g. a planet's orbital period after Gyrs of tidal evolution, and compare it to the observed value and associated uncertainty. This comparison is typically performed using a $\chi^2$ statistic if the uncertainties are assumed to be Gaussian and uncorrelated, a standard assumption. 

Since this inference requires running a \vplanet simulation, the posterior distribution is not an analytic function. Moreover, since I cannot compute $P(Data)$, so I must use a Monte Carlo sampling technique to estimate the posterior distribution. This procedure is usually performed using Markov Chain Monte Carlo (MCMC) methods such as the affine-invariant MCMC code, \emcee, whose usage is seemingly ubiquitous in modern astronomy \citep{ForemanMackey2013}. MCMC methods are incredibly powerful as they simply require computing a function that is proportional to the posterior probability, e.g. the likelihood times the prior for a given \textbf{x}, allowing them to neglect the normalization term and directly sample from the posterior distribution. How the sampling proceeds depends on the MCMC algorithm, but generally given long enough MCMC chains, the derived distributions are asymptotically guaranteed to converge to the correct distribution ensuring credible results when using an appropriate model. Standard MCMC runs often require at least $10^6$ likelihood calculations to draw a suitable number of samples from the posterior distribution and build up statistical power, but they likely require more samples depending on the dimensionality of the problem,.

A comprehensive analysis that determines whether an exoplanet is habitable requires setting many initial conditions for each of the physical modules in \vplanet, e.g. the semi-major axis, orbital eccentricity, initial water inventory, etc.  As the number of input parameters, and hence simulation dimensionality, increases, systematically exploring parameter space with simulations becomes infeasible as the number of required simulations grows exponentially with dimensionality \citep{Bellman1957}.  To handle the large dimensionality of the problem of habitability, I will exploit the methods of the burgeoning field of data science, in particular machine learning, to estimate an exoplanet's habitability.  Machine learning is a powerful application of computer science and statistics in which a probabilistic predictive algorithm learns complex patterns in data to make predictions based on what it ``learned". The first step in using a machine learning algorithm is training, i.e. calibrating a predictive model based on a subset known outcomes.  With a trained model, one can makes predictions on unseen data, i.e. unsimulated initial conditions.  A probabilistic predictive machine learning model trained on the results of \vplanet will be able to predict whether a given exoplanet is habitable or not, with an associated uncertainty, without actually performing the simulations.  These models will allow for the rapid exploration of large parameter spaces and can be used to improve the model itself to determine under what conditions that exoplanet could be habitable. 

Clearly, performing inference with theoretical models can be computationally-expensive. This issue is exacerbated by the fact that many of these models are high-dimensional, that is they require tuning many model parameters. For example to model exoplanet habitability, one must consider many different physical processes ranging from stellar evolution to atmospheric escape. Therefore, many initial conditions must be set for each simulation, e.g. an exoplanet's initial water inventory, semi-major axis, orbital eccentricity, etc.  As the number of initial conditions, and hence simulation dimensionality, increases, systematically exploring parameter space with simulations becomes computationally intractable as the number of required simulations grows exponentially with dimensionality \citep{Bellman1957}.  Not only is Bayesian inference with numerical models often slow, it is significantly hampered by the curse of dimensionality. 

Machine learning is a subset of computer science that deals with algorithms that can ``learn" complex patterns from data to make predictions (see \citet{Murphy2012} for a comprehensive review).  In a supervised framework, a machine learning algorithm ``learns" from data through a process called ``training" in which the algorithm is calibrated on a subset of the dataset with known outcomes.  By training a machine learning model on a subset of simulation initial conditions and results, one can use the calibrated model to predict results of unsimulated initial conditions, reducing computational costs.  Within the field of astronomy in the past few years, the application of supervised machine learning has become rather common. For example, \citet{Lam2018} applied supervised machine learning to successfully predict the results of complex simulations of circumbinary planet stability whereas \citet{Waldmann2016} used machine learning to predict exoplanet atmospheric chemical abundances given transmission spectra. Clearly, machine learning provides promising new methods that help predict the results of simulations and infer model parameters given data.

xxx

Furthermore, future missions such as the James Webb Space Telescope (JWST) will observe nearby planet-hosting stars in hopes of detecting the first terrestrial exoplanetary atmosphere. This search will likely focus on the inner-most planets of the nearby TRAPPIST-1 \citep{Morley2017,Lincowski2018,Lustig2019}. 

Model parameters often represent physically meaningful quantities, like an exoplanet's radius or an atmospheric chemical abundance, that we hope to infer and use to understand the underlying astrophysical processes at work. These inferences problems are not limited to detections of exoplanets or observations of their atmospheres, however, as observations of stellar systems, e.g. \kepler eclipsing binary orbital and rotation synchronization measurements \citep[e.g.][]{McQuillan2014,Lurie2017}, also require theoretical modeling of their long-term evolution for their interpretation. This paradigm of using a theoretical model for prediction, interpretation, and inference is hardly novel and has deep roots in astronomy. For example, in his famed work, the \textit{Principia}, Newton developed the well-known inverse square law model for gravity. Newton demonstrated how his model naturally produces the observed elliptical orbits of the planets in the Solar System, and hence offers a reasonable explanation for the underlying physics \citep{Newton1687,Newton1999}.

\section{Dissertation Outline}

Below, I provide an outline for this dissertation and briefly highlight my results. In Chapter 2, I study the dynamics of the birth environment of circumbinary exoplanets by running an ensemble of smooth particle hydrodynamic (SPH) N-body simulations of circumbinary protoplanetary disks. My work demonstrates that these dynamically-rich disks coevolve with their central host binary stars, exchanging angular momentum through gravitational resonances, ultimately driving the orbital evolution of both the binary and the inner-edge of the disk where planets can form and migrate. In Chapter 3, I extend my work with circumbinary planetary systems and develop a model to explain the anomalous lack of observed circumbinary planets (CBPs) orbiting short-period binary stars in the \kepler field. By constructing a theoretical model for the coupled stellar-tidal evolution of binary stars, I show that the expanding orbits of young binaries can efficiently destabilize CBPs that preferentially orbit just exterior to the dynamical stability limit, explaining their observed dearth.

I continue my work with binary stars in Chapter 4 to explore how the competition between tidal torques and magnetic braking in binaries can shape the observed rotation period distribution of low-mass main sequence stars in the \kepler field. I show how my model reproduces previously-unexplained stellar populations in the \kepler field, such as the subsynchronous eclipsing binary stars identified by \citet{Lurie2017}. I apply my theory to identify major limitations of the stellar age determination method of gyrochronology when it is erroneously applies to unresolved stellar binaries. I then explore how my model's predictions could be used in tandem with future observations to distinguish between which equilibrium tidal theory best describes tidal interactions in main sequence low-mass binary stars.

In Chapter 5, I introduce \approxposterior, my open-source machine learning Python package for approximate Bayesian inference with computationally-expensive models. I discuss the algorithm, its convergence properties, and provide example use cases. I then discuss how this efficient machine learning method can enable Bayesian inference studies for a wide array of computationally-expensive inference problems. In Chapter 6, I combine theory, Bayesian inference, and machine learning to infer the long-term high-energy radiation environment experienced by the TRAPPIST-1 planetary system, the current best target for the detection and characterization of terrestrial planet atmospheres by the James Webb Space Telescope (JWST). I construct a probabilistic model for TRAPPIST-1's evolving XUV luminosity, conditioning the inference on both observations of TRAPPIST-1 and of late M-dwarfs. From this inference, I find that TRAPPIST-1 likely underwent an extended epoch of enhanced XUV emission, potentially driving extreme volatile loss from its planets, impacting what JWST might observe in future observations. I demonstrate that this Bayesian inference analysis is too computationally-expensive to scale to either a larger number of stars or inference with a more complex model. To address this concern, I applied \approxposterior to the TRAPPIST-1 inference problem. I demonstrate that it can accurately repeat the probabilistic analysis, but requires nearly three orders of magnitude less computational resources than \emcee.  Finally, I summarize my findings and discuss prospects for related future research.

