
\section{Some Models Are Useful}

Planets are inextricably linked to their host stars through their common birth environment. The fate of binary stars is similarly intertwined.  As such, numerous coupled astrophysical processes ranging from tidal forces \citep{Zahn1989,Barnes2017} to stellar high-energy emission \citep{Airapetian2019} shape the long-term evolution of both stellar and planetary systems. Modern astronomical surveys have produced a wealth of information to help elucidate the processes that dictate this evolution. Notably, completed such as \kepler \citep{Borucki2003,Borucki2010}, \textit{K2} \citep{Howell2014}, and the in-progress Transiting Exoplanet Survey Satellite \citep[TESS, ][]{Ricker2014} have monitored the brightness variation of well over 100,000 stars to detect transiting exoplanets and eclipsing binary stars, massively increasing the known population of such systems.  These large amounts of data and the promise of future missions, e.g. the James Webb Space Telescope (JWST), present an unprecedented opportunity for both population-level statistical studies and individual target characterization to infer the fundamental properties and processes that govern the evolution stars and their planets.

An unavoidable problem for these efforts within the discipline of astronomy, however, is that the timescales over which astrophysical processes operate are often much, much longer than the lifetime of a single astronomer, or even the duration of recorded human history. For example, the main sequence lifetime of Sun-like stars is of order 10 Gyrs \citep{Baraffe2015}, a factor of $10^7$ longer than an optimistic estimate for the lifetime of a person. How can astronomers hope to learn about the long-term evolution of such objects if they can only observe brief snapshots of the cosmos over time? The fundamental problems of astronomy are therefore given what is observed, how can astronomers infer what physical processes produced the observations and what does this information imply about the past and future evolution? 

Theoretical models are critical for the interpretation of observed data and are required to understand the past and future evolution of astrophysical objects. These models provide mathematical descriptions of the underlying physical processes that permit researchers to examine how said processes operate. Researchers can then confront model predictions with data to assess their validity and characterize the underlying processes at play. For example, consider a classic problem in astronomy of understanding how and why the planets trace their apparent orbits in the night sky. In his famous work colloquially referred to as the \textit{Principia}, Newton developed the well-known inverse square law model for gravity. In his model, Newton postulated that the gravitational force that attracts massive bodies together depends on the square of their separation and the product of their masses \citep{Newton1687,Newton1999}. Newton demonstrated how his model naturally reproduces the observed elliptical orbits of the planets in the Solar System and hence offers a reasonable explanation for the underlying physics. Moreover, this model has been used to study a vast array of astrophysical phenomena including the orbits of exoplanets and galactic dynamics. Successful theoretical models offer both an explanation for the underlying processes and are able to predict the physical consequences of these mechanisms.

Theoretical models are inherently imperfect, however, as one typically must make simplifying assumptions to make evaluating the model tractable, but mostly because one often does not have a full understanding of the processes that govern the evolution of the system in question. The Newtonian theory of gravity was wildly successful\footnote{I use the inverse square law for gravity, and theories derived from it, e.g. equilibrium tides \citep{Darwin1880}, extensively in this dissertation, for example.}, but ultimately limited as it failed to explain the perihelion precession of Mercury, for example. These general issues for models are simply summarized by the well-known aphorism
\begin{quote}
All models are wrong. - \citet{Box1976}
\end{quote}
As I build theoretical models in this dissertation, I will often revisit the spirit of this quote to identify the limitations of my models and how they could be improved in future work.

Astronomers' understanding of the Universe is always advancing. Much later after Newton, Einstein identified the need for a more mathematically-complex theory to better explain the force of gravity and its consequences. In his theory of ``General Relativity" Einstein showed how a non-linear tensor field description of gravity provides a compelling explanation for numerous unexplained gravitational phenomena, including the perihelion precession of Mercury \citep{Einstein1915b,Einstein1915a}. Thus as science proceeds and observations are made, theoretical models must be derived and continually improved to interpret and explain what we see in the world around us. In this dissertation, I make much more humble theoretical contributions than the previously noted authors to understand the evolution of stars and their planets. 

\section{Much Ado About Data}

The parameters of theoretical models represent physically meaningful quantities, like a star's mass, that we hope to constrain with data and use to understand the underlying astrophysical processes at work. With data one can apply a model to infer parameters. For example in this dissertation, I apply my model for coupled stellar-tidal evolution to measurements of the orbital and rotational properties of \kepler eclipsing binaries \citep[e.g.][]{McQuillan2014,Lurie2017}. With my model, I can simulate the dynamics of such systems to constrain model parameters and understand their long-term evolution. But what about the case where astronomers have yet to observe the object or phenomenon in question?

In situations where observations have not yet been made, e.g. the detection of a terrestrial exoplanetary atmosphere, theoretical models can be used to make useful predictions.  When future facilities like JWST attempt to characterize terrestrial exoplanetary atmospheres in the search for biosignatures, for example, astronomers will use complex radiative transfer codes to estimate atmospheric compositions \citep[e.g. SMART;][]{Meadows1996,Crisp1997}. Critical to understanding and interpreting these results is determining how the exoplanet and its atmosphere evolved to its present state. This evolution necessarily depends on the long-term evolution of the planet's host star because the stellar high-energy XUV luminosity (X-ray and extreme ultraviolet emission ranging over approximately 1-1000\AA) drives volatile escape processes that have been shown to dramatically impact exoplanetary atmospheres \citep{Watson1981,Lammer2003,MurrayClay2009}. For example, \citet{Luger2015} showed via theory and simulation how the large pre-main sequence luminosities of low-mass stars can drive water photolysis and Hydrogen loss from terrestrial exoplanets during an extended runaway greenhouse phase. This photolysis can cause extreme amounts of O$_2$ to build up in the planets' atmospheres, up to 1000s of bars, significantly impacting absorption features in their spectra and the prospects for identifying true positive biosignatures \citep[see][]{Meadows2017,Meadows2018}. Modeling the long-term evolution of astrophysical systems that consider the relevant physics is critical to the estimation of their present state. Later in this dissertation, I further examine this problem by constructing a model for the long-term XUV evolution of TRAPPIST-1. I consider its impact on TRAPPIST-1's planetary system because JWST will likely focus its search on the TRAPPIST-1 planetary system \citep{Morley2017,Lincowski2018,Lustig2019}. 

When data are available, astronomers can infer information about astrophysical systems and the processes that govern them using models. Model predictions can then be conditioned on the observations to derive constraints on parameters. Many theoretical studies that examine the long-term evolution of systems to interpret observations and constrain parameters do so in the form of ``best fit models" by finding the set of model parameters that most closely reproduce the observations, within the uncertainties.  This technique is useful and informative, but ultimately insufficient, however, as a robust analysis used to interpret observations must be based in a probabilistic framework in which model predictions are conditioned on the data. Moreover, any constrained parameter requires an uncertainty estimation to permit a credible interpretation of the results.  For example, if a forward model yields a best fit prediction of 1 Earth ocean of liquid surface water remaining on an Earth-sized exoplanet, one might infer that this exoplanet could be habitable like Earth. However, how credible is this interpretation?  If instead in this example, one propagated data uncertainties through the forward model to find that the remaining surface water content is 1$^{+10}_{-1}$ Earth oceans, the planet's current state ranges from desiccated to water-rich, with both cases entirely consistent with the observations, yet indicative of significantly different evolutionary histories. Similar analogies can be made for any quantity an astronomer would like to measure, e.g. stellar masses, ages, binary orbital eccentricities, etc.

To address this issue, one can use theoretical models to constrain unobserved processes while robustly accounting for data uncertainties using the statistical method of Bayesian inference. Generally, Bayesian inference proceeds as follows:  One seeks to derive a probability distribution for model parameters, e.g. a star's mass given its observed luminosity and a model for stellar evolution.  That distribution should quantify how likely it is that the parameter takes on certain values and capture the inherent uncertainty in the parameter values given data.  This distribution is known as the ``posterior probability distribution" and accounts for observed data, the data uncertainties, parameter correlations, and one's prior belief about how the parameters are distributed. To compute the posterior distribution of model parameters, \textbf{x}, given observed data, $Data$, one applies Bayes' Theorem: 
\begin{equation} \label{intro:eqn:bayes}
P(\textbf{x} | Data) \propto P(Data | \textbf{x}) P(\textbf{x}),
\end{equation}
where $P(\textbf{x} | Data)$ is the posterior probability of \textbf{x} given $Data$, $P(\textbf{x})$ is the prior probability assigned to \textbf{x}, and $P(Data | \textbf{x})$ is the probability of the $Data$ given \textbf{x}, commonly referred to as the ``likelihood" of $Data$ given \textbf{x}. This equation neglects the normalization constant, $1/P(Data)$, as this is typically quite difficult to compute in practice because it requires integrating the likelihood over the prior. Through Bayes' Theorem, the posterior probability can be thought of as using data to update one's prior belief of how $\textbf{x}$ is distributed, given a model for the likelihood of the data. Note that in this dissertation, I will use the natural logarithm of Bayes' Theorem and refer to $\ln P(\textbf{x} | Data)$ to as the ``lnprobability". For notational convenience, I define the lnprobability as $f(\textbf{x})$. See Chapters 5 and 6 for additional discussion and a practical application of this definition. 

\section{More Ado about CPU Cycles}

In this dissertation, I built theoretical models for the long-term evolution of single and binary stars and considered what impact this evolution has on the stars' planets given available data. I implemented these models in \vplanet\footnote{VPLanet is publicly available
at \href{https://github.com/VirtualPlanetaryLaboratory/vplanet}{{https://github.com/VirtualPlanetaryLaboratory/vplanet}}.}, a modular, open-source code written in C that simulates the evolution of stellar and exoplanetary systems subject to a wide range of coupled astrophysical processes \citep{Barnes2019}. When performing Bayesian inference with \vplanet, I must compute the likelihood by running a simulation to make a prediction, e.g. a stellar binary's orbital period after Gyrs of tidal evolution, and compare it to the observed value and associated uncertainty. This comparison is typically performed using a $\chi^2$ statistic if the uncertainties are assumed to be Gaussian and uncorrelated, a standard assumption. 

Since this inference requires running a \vplanet simulation, the posterior distribution is not an analytic function. Moreover, I cannot reasonably compute the normalization term in Bayes' Theorem, $1/P(Data)$, so I must use a Monte Carlo sampling technique to estimate the posterior distribution. This procedure is usually performed using Markov Chain Monte Carlo (MCMC) methods such as the affine-invariant MCMC code, \emcee, whose usage is seemingly ubiquitous in modern astronomy \citep{ForemanMackey2013}. MCMC methods are incredibly powerful as they simply require computing a function that is proportional to the posterior probability, e.g. the likelihood times the prior for a given \textbf{x}, allowing them to neglect the normalization term and directly sample from the posterior distribution. How the sampling proceeds depends on the MCMC algorithm, but generally given long enough MCMC chains, the derived distributions are asymptotically guaranteed to converge to the correct distribution, ensuring credible results when using an appropriate theoretical model. Standard MCMC runs often require at least $10^6$ likelihood calculations to draw a suitable number of samples from the posterior distribution and build up statistical power, however. Depending on the dimensionality of the problem, i.e. how many model parameters one wishes to constrain, MCMC chains can require drawing many more samples.

Consider the coupled stellar-tidal dynamical evolution of binary stars, a case considered throughout this dissertation. Due to a combination of stellar evolution and tidal gravitational forces, stellar spins and the binary orbit evolve over long timescales, dramatically impacting their observed properties and the evolution of any planetary systems they might host \citep[for example, see][for how tides circularize binary orbits over time]{Zahn1989,Meibom2005}. A reasonable model of this evolution must at least account for stellar evolution \citep[e.g.][]{Baraffe2015}, tidal evolution \citep[e.g.][]{Hut1981,FerrazMello2008,Leconte2010}, and stellar magnetic braking \citep[e.g.][]{Matt2015}. Simulating this evolution with \vplanet would therefore require setting many initial conditions for each of the modules that control these physical effects, e.g. the stellar masses, initial orbital eccentricity, initial stellar spins, etc.  As the number of input parameters, and hence simulation dimensionality, increases, systematically exploring parameter space with simulations to understand model predictions and the underlying physics becomes infeasible as the number of required simulations grows exponentially with dimensionality, the so-called ``Curse of Dimensionality" \citep{Bellman1957}.  Not only is Bayesian inference with numerical models often slow due to the requisite number of MCMC samples, it is significantly hampered by the ``Curse of Dimensionality". Given these significant computational issues, Bayesian inference with complicated models that account for the relevant physical processes, e.g. Bayesian inference with \vplanet, often becomes computationally-intractable.

In this dissertation, I explored how the growing field of machine learning can solve these problems. Machine learning is a powerful application of computer science and statistics in which a predictive algorithm ``learns" complex patterns in data to make predictions on unseen data \citep[see][for a thorough review]{Murphy2012}. The key step in using a supervised machine learning algorithm is training, i.e. calibrating the predictive model based on a subset known outcomes.  A probabilistic predictive machine learning model trained on the results of \vplanet simulations, for example, could predict the end result of stellar-tidal evolution in binary stars, with an associated uncertainty, without actually performing the simulations.  Within the field of astronomy in the past few years, the application of supervised machine learning has become rather common. For example, \citet{Lam2018} applied supervised machine learning to successfully predict the results of complex simulations of circumbinary planet stability whereas \citet{Waldmann2016} used machine learning to predict exoplanet atmospheric chemical abundances given transmission spectra. Clearly, machine learning provides promising new methods that help predict the results of simulations and infer model parameters given data. With a trained machine learning model, I could replace running computationally-expensive simulations to enable Bayesian inference with my models, solving this issue. 

In this dissertation, I developed theoretical models to probe and infer the long-term evolution of single and binary stars given data. My research focused on the dynamical evolution of such systems and how it impacts their surrounding planetary systems. I extended my theoretical modeling to infer the evolving high-energy radiation environment experienced by the TRAPPIST-1 planetary system by comparing my models with observations of TRAPPIST-1 using Bayesian statistics. I developed and applied a novel machine learning software package to this inference problem and demonstrated its promise for enabling Bayesian inference with computationally-expensive models. Below, I provide an outline for this dissertation and briefly highlight my results. 

\section{Dissertation Outline}

In Chapter 2, I studied the dynamics of the birth environment of circumbinary exoplanets by running an ensemble of smooth particle hydrodynamic (SPH) N-body simulations of circumbinary protoplanetary disks. My work demonstrated that these dynamically-rich disks coevolve with their central host binary stars, exchanging angular momentum through gravitational resonances, ultimately driving the orbital evolution of both the binary and the inner-edge of the disk where planets can form and migrate. In Chapter 3, I extended my work with circumbinary planetary systems and developed a model to explain the anomalous lack of observed circumbinary planets (CBPs) orbiting short-period binary stars in the \kepler field. By constructing a theoretical model for the coupled stellar-tidal evolution of binary stars, I showed that the expanding orbits of young binaries can efficiently destabilize CBPs that preferentially orbit just exterior to the dynamical stability limit, explaining their observed dearth.

I continued my work with binary stars in Chapter 4 to explore how the competition between tidal torques and magnetic braking in binaries can shape the observed rotation period distribution of low-mass main sequence stars in the \kepler field. I showed how my model reproduces previously-unexplained stellar populations in the \kepler field, such as the subsynchronous eclipsing binary stars identified by \citet{Lurie2017}. I applied my theory to identify major limitations of the stellar age determination method of gyrochronology when it is erroneously applied to unresolved stellar binaries. I then explored how my model's predictions could be used in tandem with future observations to distinguish between which equilibrium tidal theory best describes tidal interactions in main sequence low-mass binary stars.

In Chapter 5, I introduced \approxposterior, my open-source machine learning Python package for approximate Bayesian inference with computationally-expensive models. I discussed the algorithm, its convergence properties, and provided example use cases. I then discussed how this efficient machine learning method can enable Bayesian inference studies for a wide array of computationally-expensive inference problems. In Chapter 6, I combined theory, Bayesian inference, and machine learning to infer the long-term high-energy radiation environment experienced by the TRAPPIST-1 planetary system, the current best target for the detection and characterization of terrestrial planet atmospheres by JWST. I constructed a probabilistic model for TRAPPIST-1's evolving XUV luminosity, conditioning the inference on both observations of TRAPPIST-1 and of late M-dwarfs. From this inference, I found that TRAPPIST-1 likely underwent an extended epoch of enhanced XUV emission, potentially driving extreme volatile loss from its planets, impacting what JWST might observe in future observations. I demonstrated that this Bayesian inference is too computationally-expensive to scale to either a larger number of stars or inference with a more complex model. To address this concern, I applied \approxposterior to the TRAPPIST-1 inference problem. I demonstrated that it can accurately repeat the probabilistic analysis, but required nearly three orders of magnitude less computational resources than \emcee.  Finally, I summarized my findings and discussed prospects for related future research.

